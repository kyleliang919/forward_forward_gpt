# forward_forward_gpt
## Description

Exploring training large language model with [Hinton's forward forward algorithm](https://www.cs.toronto.edu/~hinton/FFA13.pdf).

Based on my rudimentary understanding of the forward forward algorithm, it's a "GAN" with generator and discriminator sharing the same encoder, which is being trained only with the discriminator objective. The "goodness" function is equivalent to a per layer discriminator loss function, where negative sample (in our case generated by GPT itself) 's "goodness" minimize and maximize the positive activation.

## Problems
* Forward forward can't train embedding, simply because there is not really a difference between "positive" and "negative", the current hack is adding another "skip connection" from the embeddings directly to the end loss.

### Dependencies

* transformer
* torch
* tqdm


### Run Example

```
python src/main.py
```

## Authors
[@KyleLiang5](https://twitter.com/KyleLiang5)

## License

This project is licensed under the MIT License - see the LICENSE.md file for details

## Acknowledgments